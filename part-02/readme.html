<!DOCTYPE html><html><head><meta charset="utf-8"><style>body {
  max-width: 980px;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headeranchor-link {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .headeranchor-link:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .headeranchor,
.markdown-body h2 .headeranchor,
.markdown-body h3 .headeranchor,
.markdown-body h4 .headeranchor,
.markdown-body h5 .headeranchor,
.markdown-body h6 .headeranchor {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .headeranchor-link,
.markdown-body h2:hover .headeranchor-link,
.markdown-body h3:hover .headeranchor-link,
.markdown-body h4:hover .headeranchor-link,
.markdown-body h5:hover .headeranchor-link,
.markdown-body h6:hover .headeranchor-link {
  height: 1em;
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .headeranchor-link .headeranchor,
.markdown-body h2:hover .headeranchor-link .headeranchor,
.markdown-body h3:hover .headeranchor-link .headeranchor,
.markdown-body h4:hover .headeranchor-link .headeranchor,
.markdown-body h5:hover .headeranchor-link .headeranchor,
.markdown-body h6:hover .headeranchor-link .headeranchor {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* Multimarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\f05c';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><title>readme</title></head><body><article class="markdown-body"><h1 id="readme"><a name="user-content-readme" href="#readme" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Readme</h1>
<p><strong>Note</strong>: This project was conducted in Jupyter notebooks running Python 3.6 kernels. All python packages installed on the team&rsquo;s environment and their corresponding versions are listed in <strong>requirements.txt</strong>.</p>
<p>For code and additional comments, please see the individual <strong>notebooks</strong> and <strong>scripts</strong>.</p>
<h1 id="a-comparison-of-recommendation-models-for-amazons-movie-and-tv-catalog-part-ii"><a name="user-content-a-comparison-of-recommendation-models-for-amazons-movie-and-tv-catalog-part-ii" href="#a-comparison-of-recommendation-models-for-amazons-movie-and-tv-catalog-part-ii" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>A Comparison of Recommendation Models for Amazon&rsquo;s Movie and TV Catalog - Part II</h1>
<h4 id="by-charissa-ding-carrie-yang-and-derek-zhao"><a name="user-content-by-charissa-ding-carrie-yang-and-derek-zhao" href="#by-charissa-ding-carrie-yang-and-derek-zhao" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>by Charissa Ding, Carrie Yang, and Derek Zhao</h4>
<h2 id="abstract"><a name="user-content-abstract" href="#abstract" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Abstract</h2>
<p>This portion of the project is an indirect extension of the previous work performed in <a href="https://github.com/zhao1701/amazon-movie-recommendation/tree/master/part-01#a-comparison-of-recommendation-models-for-amazons-movie-and-tv-catalog">Part I</a>. The premise for Part 2 remains largely the same: given a dataset from an online retailer describing how various users have rated various items, we seek to accurately infer how users will rate items they have not previously consumed in order to recommend products we believe a user would rate highly, thus constructing a personalized recommendation system that will increase revenue and customers&rsquo; trust in the retailers&rsquo; recommendations. Once again, we use data consisting of customers&rsquo; explicit one-to-five-star ratings of Amazon&rsquo;s film and TV product catalog, the details and exploratory analysis of which were previously described in <a href="https://github.com/zhao1701/amazon-movie-recommendation/tree/master/part-01#data">Part I - Data</a>.</p>
<p><strong>Note</strong>: For the remainder of this project, the term <strong>ratings</strong> will refer to these star ratings which are assigned on a whole number scale from 1 to 5 inclusive.</p>
<p>Whereas Part I explores the efficacy of a broad array of conventional recommendation models across datasets of varying sizes and evaluates model performance using numerous metrics, Part 2 focuses in-depth on optimizing the performance of a number of more modern techniques. To this end, for model comparison, we use only one sample of the dataset rather than many, and we evaluate the models on fewer metrics.</p>
<p>In exploring some of the less conventional recommendation algorithims and discovering how they may or may not help improve recommendation performance, we implement a number of models ourselves in order to better and more deeply understand how they function. In particular, we build a Locality Sensitive Hashing model with Cosine Similarity (CosineLSH), and a Probabilistic Latent Semantic Indexing (PLSI) model.</p>
<p>In addition, we also utilize the factoriztion machine model from the FastFM library, and experiment with a few different feature selection schemes, in search for a model with optimal performance. In the end, we find that, a factorization machine that takes the predictions from the LSH and ALS baseline model as features, combined with additional item features such as list of actors, directors, production studios, and box office, leads to the best performance.</p>
<h2 id="data-sampling"><a name="user-content-data-sampling" href="#data-sampling" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Data Sampling</h2>
<p>In Part I, in order to evaluate how the effectiveness of each model changes with varying amounts of ratings data, we sample the original dataset to produce sample sets of different sizes. The sample set names and data sampling methodology was <a href="https://github.com/zhao1701/amazon-movie-recommendation/tree/master/part-01#data-sampling">described previously</a>. Because Part 2 focuses on evaluating model performance on a fixed dataset, this portion of the project uses only data from the <strong>Ratings18</strong> sample set. It constitutes 18% of all ratings data for Amazon&rsquo;s film and TV catalog and specifically includes only items that have been reviewed at least 20 times and users that have given at least 20 reviews.</p>
<h2 id="evaluation-metrics-and-objective"><a name="user-content-evaluation-metrics-and-objective" href="#evaluation-metrics-and-objective" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Evaluation Metrics and Objective</h2>
<p>Below we define the metrics used to evaluate the performance of each predictive model. In Part I, the evaluation metrics are chosen and defined presuming that a recommendation system functions as follows:</p>
<ol>
<li>Given a user, the system uses a previously trained model to predict how that user will rate all items he has not yet rated.</li>
<li>All unrated items are ranked by their predicted ratings, and the top <em>k</em> items are recommended to the user.</li>
</ol>
<p>The obvious benefits of such a system are that the items with the highest predicted ratings are most likely to be relevant to the user, and thus the quality of those top <em>k</em> predictions is presumed to be high. However, in Part I, we see that the tradeoff is a restricted subset of items are continually recommended to the users. This is disadvantageous for the retailer as a large proportion of items in its catalog are never recommended even once, and this is unfortunate for the customer who might tire of seeing the same recommendations repeatedly. Therefore, in Part 2, we consider an alternate system:</p>
<ol>
<li>Given a user, the system uses a previously trained model to predict how that user will rate all items he has not yet rated.</li>
<li>Of the unrated items with predicted ratings of 4 stars of above (out of a maximum 5), <em>k</em> items are randomly recommended to the user.</li>
</ol>
<p>While this system might be less precise in its recommendations, both the retailer and the user benefit from being exposed to a much wider variety of recommendations. In this context, the following criteria are particularly relevant:</p>
<ol>
<li><strong>Mean absolute error (MAE)</strong>: MAE is the average of the absolute value of the difference between a predicted user-item rating and the actual user-item rating. It is a useful metric for understanding how close the typical predicted rating is from the actual rating. This is the primary metric by which models are tuned and their accuracy/predictive power judged.</li>
<li><strong>Area Under the Receiver Operating Characterstic curve (AUC)</strong>: The <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">ROC curve</a> is used to illustrate the discriminative power of a binary classifier and its <strong>AUC</strong> score is a useful single number summary of said power. While the models used to predict ratings are regression models, the predicted ratings must be binarized before recommendations are made (recall ratings of 4 or above are considered relevant and ratings below 4 are considered not relevant). Thus, the quality of a model&rsquo;s rating predictions can also be judged by how useful those predictions are in discriminating between relevant and not relevant items.</li>
<li><strong>Catalog coverage</strong>: Catalog coverage is the number of items that are recommended at least once divided by the total number of items in the catalog. For an online retailer, maximizing catalog coverage is desirable as a larger variety of products can be surfaced through recommendation, thereby increasing the chances that a customer may discover a movie or TV show genuinely new and unexpected.</li>
<li><strong>User coverage</strong>: User coverage is the number of users to whom a recommendation of a predicted relevant item is made at least once. Hypothetically, if a system predicts no items are relevant for a user, the system can still recommend the top <em>k</em> items with the highest predicted ratings. However, it remains useful to know for what proportion of users such a backup method would need to be implemented.</li>
</ol>
<h2 id="models"><a name="user-content-models" href="#models" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Models</h2>
<p>In the context of a recommendation problem, a predictive model is an algorithm capable of using existing user-item ratings, such as those already present in the dataset, to infer missing user-item ratings (i.e. how a user would rate an item they have not actully rated). For the problem of generating recommendations from the Amazon Movie and TV Reviews dataset, we consider the following models.</p>
<ol>
<li><strong>Probabilistic latent semantic indexing (PLSI)</strong>: <a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">PLSI</a> is a probabilistic method that models the probability of co-occurence between users and items. In the context of ratings and recommendations, a user-item pair is considered co-occurrent if the item is relevant to the user. Unlike the other 7 models, PLSI accepts as input and returns as output only binary implicit ratings. </li>
<li><strong>Averaging baseline</strong>: The averaging baseline simply calculates the overall average of all ratings in the training set and uses this value as the prediction for all missing ratings. It is the crudest of all models.</li>
<li><strong>Bias baseline</strong>: In Part I, we see that the bias model trained using alternating least squares achieves the lowest MAE of the models tested. We include this model as a benchmark. Bias parameters are learned for each user and each item, and a user-item rating is modeled as the sum of the mean rating, the user bias, and the item bias.</li>
<li><strong>Item-based collaborative filtering with cosine-based locality sensitive hashing (LSH)</strong>: <a href="https://en.wikipedia.org/wiki/Item-item_collaborative_filtering">Item-based collaborative filtering</a> is a predictive method where the inferred rating for a user-item pair is calculated as a linear combination of the ratings for the most similar items that user has already rated. The question of deciding which items are most similar to another can be addressed using <a href="https://stackoverflow.com/questions/12952729/how-to-understand-locality-sensitive-hashing">cosine-based LSH</a> to determine approximate nearest neighbors.</li>
<li><strong>Factorization machine (FM)</strong>: <a href="https://getstream.io/blog/factorization-recommendation-systems/">FM&rsquo;s</a> model interactions between features through using factorized parameters. They are extremely effective on sparse data and provide a convenient means of including additional features into the dataset beyond the standard (user, item, rating) tuple. We test three FM models, each with different variations in feature engineering.</li>
<li><strong>Ensemble with gradient boosted trees (GBT)</strong>: As the various previously mentioned models are expected to have different strengths and weaknesses in their ability to predict ratings, we use a standard ensembling technique of using their outputs as inputs into a <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient tree boosting </a> model for improved performance.</li>
</ol>
<h2 id="methodology"><a name="user-content-methodology" href="#methodology" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Methodology</h2>
<h4 id="data-splits"><a name="user-content-data-splits" href="#data-splits" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Data Splits</h4>
<p>The <strong>Ratings18</strong> sample set consists of 441,878 ratings for 7363 items from 7508 users. Because FM&rsquo;s allow for the incorporation of extra features, additional information can be scraped from various sources described later. However, because this additional data is not available for a subset of 600 items, 30,000 ratings for these items are excluded. This results in a dataset of 414,452 ratings for 6765 items from 7508 users.</p>
<p>This dataset is then shuffled and split 80%-10%-10% into a training set, cross-validation set, and test set. The ratio of the split may seem unusual given that more traditional splits allocate more data to the cross-validation and training sets. However, with over 4 million total ratings, we believe over 40,000 ratings provides enough data for cross-validation and testing to be just as effective.</p>
<h4 id="hyperparameter-search-and-model-tuning"><a name="user-content-hyperparameter-search-and-model-tuning" href="#hyperparameter-search-and-model-tuning" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Hyperparameter Search and Model Tuning</h4>
<p>For hyperparameter searches, we eschew the traditional brute force method of grid searching for optimal hyperparameters and instead turn to a form of Bayesian optimization utilizing <a href="http://steventhornton.ca/hyperparameter-tuning-with-hyperopt-in-python/">tree-structured Parzen estimators (TPE)</a>. It has been show that Bayesian optimization is a more effective model tuning method than random search and that random search is a more effective model tuning method than grid search. Bayesian optimization works by iteratively making a guess about the optimal combination of hyperparameter values, testing that guess by evaluating a model with those hyperparameter values, and based on the model&rsquo;s performance, positing an updated guess. In this way, it is able to converge faster to optimal hyperparameter values than grid search or random search. For each hyperparameter search in this project 100 evaluations are made in which a model is fit on a training set, its performance is measured as its MAE on the cross-validation set, and an updated hyperparameter setting is computed for the next evaluation.</p>
<h4 id="probabilistic-latent-semantic-indexing"><a name="user-content-probabilistic-latent-semantic-indexing" href="#probabilistic-latent-semantic-indexing" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Probabilistic Latent Semantic Indexing</h4>
<p>PLSI models the co-occurence of users and items by introducing latent factors representing concepts akin to user communities and item genres. Intuitively, if a user is modeled as having a high probability of belonging to a certain genre/community/latent factor, then all items modeled as having a high probability of belonging to that same genre/community/latent factor have a high probability of co-occuring with that user (i.e. it is likely that user would have rated those items 4 or above). </p>
<p>Ratings data is binarized such that each rating of 4 or above represents a co-occurence for that user-item pair. The binarized ratings, along with user and item information, serve as the data upon which the PLSI model learns. The PLSI model is capable of two predictions:</p>
<ol>
<li>The conditional probability that a specific item is rated 4 or above given the user is known.</li>
<li>The joint probability that a user-item pair has a rating of 4 or above.</li>
</ol>
<p>The first type prediction is most useful for making top <em>k</em> recommendations, as the conditional probabilities for all items given a user can be ranked. However, for the purposes of binary classification, we use the second type of prediction, presuming that a threshold can be found such that most probabilities above said threshold represent ratings of 4 or above and most ratings below said threshold represent ratings below 4.</p>
<p>We use <a href="https://github.com/zhao1701/amazon-movie-recommendation/blob/master/part-02/03%20-%20Probabilistic%20Latent%20Semantic%20Indexing.ipynb">our own implementation of the PLSI algorithm</a>, optimizing latent factor parameters using <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation maximization (EM)</a>. The EM algorithm&rsquo;s main benefit is that it is highly parallelizable, which given the computing resources available for this project, we could not capitalize on. Unfortunately, without parallelization, the EM algorithm is impractically slow to converge. Nevertheless, we perform a basic hyperparameter grid search to assess the performance of PLSI.</p>
<p>Specifically, the only two hyperparameters we implemented are:</p>
<ol>
<li><strong>n_factors</strong>: The number of latent factors used to model co-occurences.</li>
<li><strong>n_iterations</strong>: The number of times the EM algorithm is called to update latent factor parameters.</li>
</ol>
<p>As the PLSI model&rsquo;s output is a series of probabilities, it is not a regression model. Thus, we use the AUC score to assess the model&rsquo;s performance. The heatmap below shows that while 30 latent factors and a high number of iterations is optimal, the AUC score is only a quite poor 0.6. Even though the PLSI model performs well on the most basic of test cases, Amazon&rsquo;s ratings data may be too noisy for a basic PLSI implementation to succeed, so further exploration is discontinued.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-plsi.png" /></p>
<h4 id="item-based-collaborative-filtering-with-cosine-based-locality-sensitive-hashing"><a name="user-content-item-based-collaborative-filtering-with-cosine-based-locality-sensitive-hashing" href="#item-based-collaborative-filtering-with-cosine-based-locality-sensitive-hashing" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Item-based Collaborative Filtering with Cosine-based Locality Sensitive Hashing</h4>
<p>In item-based collaborative filtering, a predicted rating for a user-item pair is calculated as a linear combination of the ratings for the most similar items that user has already rated. However, calculating and ranking pairwise similarities amongst all items can become impractical given a dataset of sufficiently large size. One solution is to determine the approximate nearest neighbors of each item and use only these neighbors (or a subset) when predicting a rating for a user-item pair.</p>
<p>Because each item can be represented as a vector of explicit ratings from various users, the similarity of two items can be measured using cosine similarity. Cosine-based LSH is a hashing technique wherein the probability that two item-vectors are hashed to the same key is proportional to their cosine similarity. If a hash-signature is built with multiple keys, then only those items that are highly similar will have the same signature. If multiple signatures are created per item, and two items are considered neighbors if at least one of those signatures is the same, then items that are somewhat similar still have some probability of being considered neighbors.</p>
<p>Once approximate nearest neighbors are calculated for all items, for each unrated item of a particular user, we can use that item&rsquo;s nearest neighbors that the user has rated to infer a predicted rating:</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/cosineLSH-formula-2.png" /></p>
<p>where <em>u</em> is the user, <em>i</em> is the item for which a rating is to be predicted, and <em>sim</em> is the cosine similarity of two items based on the ratings given from users that have rated both items. </p>
<p><a href="https://github.com/zhao1701/amazon-movie-recommendation/blob/master/part-02/04%20-%20Item-based%20Collaborative%20Filtering%20with%20Cosine%20LSH.ipynb">Our implementation of item-based CF with cosine LSH</a> has two hyperparameters: <strong>p</strong> and <strong>q</strong>. Their descriptions and optimal values, found through a TPE hyperparameter search, are displayed below.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Description</th>
<th>Optimal Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>p</td>
<td>The number of keys per signature</td>
<td>3</td>
</tr>
<tr>
<td>q</td>
<td>The number of signatures per item</td>
<td>25</td>
</tr>
</tbody>
</table>
<p>Data from the full search is plotted below.</p>
<ul>
<li>The first plot of each row shows the hyperparameter value tested at each iteration. As TPE discovers which value is more effective, it narrows its search region, which is more evident in later hyperparameters.</li>
<li>The second plot of each row shows the frequency with which a particular hyperparameter value is tested. TPE tends to sample more frequently from regions where model performance is better. </li>
<li>The third plot of each row shows the various loss values associated with a given hyperparameter value. The loss value is MAE of the model&rsquo;s predictions on the cross-validation set using a given hyperparameter configuration.</li>
</ul>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/cosineLSH-params-1.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/cosineLSH-params-2.png" /></p>
<p>The optimal hyperparameter values are unsurprising: lower values for <strong>p</strong> decrease the number of neighborhoods while increasing their size (reducing the approximation aspect of approximate nearest neighbors at the expense of calculating more similarities) while higher values of <strong>q</strong> have the same effect.</p>
<h4 id="bias-baseline"><a name="user-content-bias-baseline" href="#bias-baseline" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Bias Baseline</h4>
<p>The bias baseline is a learned baseline model that calculates a predicted rating using the following formula:</p>
<pre><code>predicted rating = overall average rating + user bias + item bias
</code></pre>
<p>Instead of explicitly calculating values for each user and item bias, the model treats user and item biases as parameters to be learned through optimizing an objective function, with alternating least squares (ALS) as the chosen method of optimization for its fast convergence.</p>
<p>Below are the results of a TPE search for optimal hyperparameter values of the bias model.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Description</th>
<th>Optimal Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>n_epochs</td>
<td>Number of times parameters are updated through ALS</td>
<td>6</td>
</tr>
<tr>
<td>reg_i</td>
<td>L2 penalty weight for item bias.</td>
<td>0</td>
</tr>
<tr>
<td>reg_u</td>
<td>L2 penalty weight for user bias.</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-bias-1.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-bias-2.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-bias-3.png" /></p>
<h4 id="factorization-machine-with-only-ratings-data"><a name="user-content-factorization-machine-with-only-ratings-data" href="#factorization-machine-with-only-ratings-data" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Factorization Machine with Only Ratings Data</h4>
<p>As noted previously, FM&rsquo;s model interactions between features using factorized parameters. Although FM&rsquo;s are especially useful for incorporating external data as features, we first build an FM model that considers only user, item, and ratings data.</p>
<p>Below are the results of a TPE search for optimal hyperparameter values of this FM model.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Description</th>
<th>Optimal Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>init_stdev</td>
<td>The standard deviation of initialized parameters</td>
<td>0.765</td>
</tr>
<tr>
<td>l2_reg_V</td>
<td>L2 penalty weight for pairwise coefficients</td>
<td>7.079</td>
</tr>
<tr>
<td>l2_reg_w</td>
<td>L2 penalty weight for linear coefficients</td>
<td>0.861</td>
</tr>
<tr>
<td>n_iter</td>
<td>Number of ALS parameter updates</td>
<td>929</td>
</tr>
<tr>
<td>rank</td>
<td>The rank of the factorization used for the second order interactions</td>
<td>2</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-ratings-params-1.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-ratings-params-2.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-ratings-params-3.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-ratings-params-4.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-ratings-params-5.png" /></p>
<p>We see above that for ratings only data, little regularization of linear coefficients ought to be applied while significant regularization of second-order coefficients is necessary. Furthermore, it makes sense that given the sparsity of the data, only a rank of 2 is needed, as interactions above the second-order do not exist.</p>
<h4 id="factorization-machine-with-lsh-and-bias-baseline-data"><a name="user-content-factorization-machine-with-lsh-and-bias-baseline-data" href="#factorization-machine-with-lsh-and-bias-baseline-data" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Factorization Machine with LSH and Bias Baseline Data</h4>
<p>This FM extends the previous FM model by adding the predicted ratings generated from the item-based CF with LSH model and a simple bias model as features in the FM model. This ensembling technique will later be compared with externally blending the outputs of an FM, CF, and bias model rather than feeding the outputs of the CF and bias models into the FM.</p>
<p>Below are the results of a TPE search for optimal hyperparameter values of this FM model.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Description</th>
<th>Optimal Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>init_stdev</td>
<td>The standard deviation of initialized parameters</td>
<td>0.442</td>
</tr>
<tr>
<td>l2_reg_V</td>
<td>L2 penalty weight for pairwise coefficients</td>
<td>7.360</td>
</tr>
<tr>
<td>l2_reg_w</td>
<td>L2 penalty weight for linear coefficients</td>
<td>1.501</td>
</tr>
<tr>
<td>n_iter</td>
<td>Number of ALS parameter updates</td>
<td>883</td>
</tr>
<tr>
<td>rank</td>
<td>The rank of the factorization used for the second order interactions</td>
<td>2</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-lsh-als-params-1.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-lsh-als-params-2.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-lsh-als-params-3.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-lsh-als-params-4.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-lsh-als-params-5.png" /></p>
<p>We see above that the optimal hyperparameter values for this FM model are not much different from the previous. However, the regularization of linear coefficients is increased to compensate for the presence of new features on which the model risks overfitting.</p>
<h4 id="factorization-machine-with-external-data"><a name="user-content-factorization-machine-with-external-data" href="#factorization-machine-with-external-data" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Factorization Machine with External Data</h4>
<p>Because FM&rsquo;s allow the incorporation of additional features beyond what already exists in the Amazon Product Data used for Part I, namely user (reviewerID), item (ASIN - Amazon Standard Identification Number), and rating, we perform web scraping from different online sources in order to collect additional information about each item to feed into the factorization machine. Specifically, we collect new features from the following three sources:</p>
<ol>
<li>Amazon</li>
<li>OMDB (The Open Move Database)</li>
<li>IMDB (The Internet Movie Database)</li>
</ol>
<p>For each database, we use their corresponding Python API to collect additional data. We first match items to data in the Amazon database using their ASIN. Because both IMDB and OMDB do not contain ASIN&rsquo;s as features, we use the items&rsquo; product titles and release years collected from Amazon to search for and retrieve data from IMDB and OMDB. Because the items in our dataset include both TV series and films while IMDB and OMDB focus primarily on film data, approximately 600 items have no corresponding data in IMDB or OMDB. For the sake of consistency when comparing models, we exclude all data involving these 600 items, as described earlier.</p>
<p>Between the three sources of data, the following features are collected and considered for feature engineering:</p>
<ul>
<li><strong>box office</strong>: The amount of revenue a film has earned during its theatrical release.</li>
<li><strong>country</strong>: The countries in which the item has been released.</li>
<li><strong>language</strong>: The languages in which the item has been released.</li>
<li><strong>metascore</strong>: The <a href="http://www.metacritic.com/">metacritic</a> score for that item (on a scale of 0 to 100).</li>
<li><strong>mpaa rating</strong>: The maturity rating of that item (ex: PG, PG-13, R, TV-MA, etc.)</li>
<li><strong>runtime</strong>: The total running time of that item.</li>
<li><strong>type</strong>: Whether the item is a film or TV series.</li>
<li><strong>year</strong>: The year in which the item was released.</li>
<li><strong>vfx (visual effect)</strong>: Whether the crew for that item included a visual effects department.</li>
<li><strong>imdb genre</strong>: The genres the item belongs to (ex: sci-fi, romance, comedy, etc).</li>
<li><strong>imdb studios</strong>: The production studios that created the item. This data is converted into a lemmatized <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag of words</a> model to account for varying studio naming conventions.</li>
<li><strong>imdb rating</strong>: The average rating on a scale of 1 to 10 that IMDB users assigned to the item.</li>
<li><strong>imdb votes</strong>: The number of IMDB users that rated the item.</li>
<li><strong>directors</strong>: The directors for the item.</li>
<li><strong>amazon genre</strong>: The primary genre the item belongs to (ex: sci-fi, romance, comedy, etc).</li>
<li><strong>actors</strong>: The main actors that appear in the item.</li>
<li><strong>amazon studio</strong>: The primary production studio that created the item. This data is similarly converted to a bag of words model.</li>
<li><strong>amazon sales rank</strong>: The current sales rank of that item.</li>
</ul>
<p>In total, <strong>18</strong> features for <strong>6765</strong> items are collected. This data is then merged with the original user-item ratings data, using <strong>ASIN</strong> as the primary key to ensure accurate matching.</p>
<p>Feature engineering is automated as part of the hyperparameter search by treating the decision of whether or not to include a feature in the dataset as a hyperparameter. Below are the results of a TPE search for optimal hyperparameter values of this FM model.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Description</th>
<th>Optimal Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>n_iter</td>
<td>Number of ALS parameter updates</td>
<td>997</td>
</tr>
<tr>
<td>init_stdev</td>
<td>The standard deviation of initialized parameters</td>
<td>0.20</td>
</tr>
<tr>
<td>rank</td>
<td>The rank of the factorization used for the second order interactions</td>
<td>2</td>
</tr>
<tr>
<td>l2_reg_w</td>
<td>L2 penalty weight for linear coefficients</td>
<td>16.22</td>
</tr>
<tr>
<td>l2_reg_v</td>
<td>L2 penalty weight for pairwise coefficients</td>
<td>20.97</td>
</tr>
<tr>
<td>use_actors</td>
<td>Whether to include actors as a feature in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_country</td>
<td>Whether to include country as a feature in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_directors</td>
<td>Whether to include directors as a feature in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_genres</td>
<td>Whether to include genres as a feature in the FM model</td>
<td>False</td>
</tr>
<tr>
<td>use_language</td>
<td>Whether to include language as a feature in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_mpaa</td>
<td>Whether to include mpaa rating as a feature in the FM model</td>
<td>False</td>
</tr>
<tr>
<td>use_studios</td>
<td>Whether to include production studios as a feature in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_type</td>
<td>Whether to include item type (movie, episode, series, game) as a feature in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_scores</td>
<td>Whether to include metascore and imdb score as features in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_popularity</td>
<td>Whether to include popularity metrics (imdb votes, Amazon sales rank, and box office) as features in the FM model</td>
<td>True</td>
</tr>
<tr>
<td>use_year</td>
<td>Whether to include release year as a feature in the FM model</td>
<td>False</td>
</tr>
<tr>
<td>use_model_results</td>
<td>Whether to include results from LSH and bias baseline models as features in the FM model</td>
<td>True</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-1.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-2.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-3.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-4.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-5.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-6.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-7.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-8.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-9.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-10.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-11.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-12.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-13.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-14.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-15.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-16.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/fm-external-params-17.png" /></p>
<p>With the exception of genres, release year, and MPAA rating, all other features contribute to the best model the hyperparameter search can find. Interestingly, despite the inclusion of a number of new features, an FM model with a rank of 2 still provides the best results, provided a high degree of regularization is used for both linear and second-order coefficients.</p>
<h4 id="gradient-boosted-tree-ensemble"><a name="user-content-gradient-boosted-tree-ensemble" href="#gradient-boosted-tree-ensemble" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Gradient Boosted Tree Ensemble</h4>
<p>Whereas the FM with LSH and Bias Baseline model feeds the predicted ratings of the item-based CF model and bias baseline model as features in the FM model, this ensembling method treats the predicted ratings of the ratings-only-FM model, item-based CF model, and bias baseline model as features in a <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient tree boosting</a> model.</p>
<p>Below are the results of a TPE search for optimal hyperparameter values of the GBT ensemble.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Description</th>
<th>Optimal Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>learning_rate</td>
<td>Rate at which coefficients are updated</td>
<td>0.119</td>
</tr>
<tr>
<td>max_depth</td>
<td>Maximum tree depth for base learners</td>
<td>1</td>
</tr>
<tr>
<td>n_estimators</td>
<td>Number of boosted trees to fit</td>
<td>94</td>
</tr>
<tr>
<td>reg_alpha</td>
<td>L1 regularization term on coefficients</td>
<td>2.710</td>
</tr>
<tr>
<td>reg_lambda</td>
<td>L2 regularization term on coefficients</td>
<td>1.457</td>
</tr>
<tr>
<td>subsample</td>
<td>Proportion of training data sampled for each tree</td>
<td>0.429</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-ensemble-1.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-ensemble-2.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-ensemble-3.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-ensemble-4.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-ensemble-5.png" /><br />
<img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/hp-ensemble-6.png" /></p>
<h2 id="results"><a name="user-content-results" href="#results" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Results</h2>
<p>Below we compare the performance of each optimally tuned model along several metrics. Unless explicitly stated, all results are metrics obtained from evaluating model performance on the test set.</p>
<h4 id="mean-absolute-error"><a name="user-content-mean-absolute-error" href="#mean-absolute-error" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Mean Absolute Error</h4>
<p>Using the average training set rating as a prediction for all ratings results in a test MAE of 0.94, essentially 1 star. Against this baseline, collaborative filtering with LSH performs noticeably better, with a test MAE of 0.79, or 4/5ths of a star. While such an MAE is not close to that of the best models, it remains acceptable considering that the CF model uses an approximation algorithm for finding nearest neighbors, and so is not expected to be as accurate as a pure collaborative filtering with KNN model.</p>
<table>
<thead>
<tr>
<th></th>
<th>MeanBaseline</th>
<th>CF with LSH</th>
<th>BiasBaseline</th>
<th>FM with RatingsOnly</th>
<th>FM with LSH+Bias</th>
<th>FM with ExternalData</th>
<th>GBT Ensemble</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>0.941307</td>
<td>0.776285</td>
<td>0.690115</td>
<td>0.642497</td>
<td>0.645602</td>
<td>0.650397</td>
<td>0.634640</td>
</tr>
<tr>
<td>CV</td>
<td>0.938894</td>
<td>0.790865</td>
<td>0.725451</td>
<td>0.717786</td>
<td>0.719750</td>
<td>0.709635</td>
<td>0.717471</td>
</tr>
<tr>
<td>Test</td>
<td>0.941661</td>
<td>0.787686</td>
<td>0.718741</td>
<td>0.712174</td>
<td>0.712980</td>
<td>0.705646</td>
<td>0.711717</td>
</tr>
</tbody>
</table>
<p>The bias baseline model still performs remarkably well given its computational simplicity, however it is outperformed by all FM models and the GBT ensemble. Among the FM and ensemble models, it is interesting that the FM that uses CF with LSH and bias baseline predictions as input features performs worse than just an FM using only basic ratings data. However, if predictions from the ratings-only FM are ensembled with CF and bias baseline predictions, performance is slightly improved over that of the ratings-only FM.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/mae-results.png" /></p>
<p>Finally, it appears that incorporating external data into an FM does indeed lead to improved performance, yielding a test MAE of 0.7056, the lowest MAE of both Part I and Part II of this project.</p>
<h4 id="area-under-receiver-operating-characteristic-curve"><a name="user-content-area-under-receiver-operating-characteristic-curve" href="#area-under-receiver-operating-characteristic-curve" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Area Under Receiver Operating Characteristic Curve</h4>
<p>As stated previously, the AUC score provides a means of assessing the utility of the predicted ratings in discriminating between relevant (rated 4 stars or above) and not relevant items (rated below 4 stars). An AUC score of 0.5 suggests no discriminating power while an AUC score of 1 represents maximum discriminating power. We expect that models with lower test MAE&rsquo;s also have higher AUC scores, and this is largely the case, with the FM model using external data yielding the highest AUC score of 0.827.</p>
<table>
<thead>
<tr>
<th></th>
<th>MeanBaseline</th>
<th>CF with LSH</th>
<th>BiasBaseline</th>
<th>FM with RatingsOnly</th>
<th>FM with LSH+Bias</th>
<th>FM with ExternalData</th>
<th>GBT Ensemble</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>0.5</td>
<td>0.771787</td>
<td>0.842967</td>
<td>0.871505</td>
<td>0.871109</td>
<td>0.861593</td>
<td>0.87213</td>
</tr>
<tr>
<td>CV</td>
<td>0.5</td>
<td>0.762181</td>
<td>0.814528</td>
<td>0.816789</td>
<td>0.816938</td>
<td>0.823384</td>
<td>0.816143</td>
</tr>
<tr>
<td>Test</td>
<td>0.5</td>
<td>0.767861</td>
<td>0.820651</td>
<td>0.821890</td>
<td>0.822598</td>
<td>0.827128</td>
<td>0.821585</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/auc-results.png" /></p>
<h4 id="roc-and-precision-recall-curves"><a name="user-content-roc-and-precision-recall-curves" href="#roc-and-precision-recall-curves" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>ROC and Precision-Recall Curves</h4>
<p>The ROC and precision-recall curves for the test performance of each model suggests that the predictions made by the bias baseline, FM models, and ensemble are very similar. That is, while some models within the aforementioned group perform marginally better than others, they tend to make similar mistakes and exhibit the same degrees of tradeoff between precision and recall.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/roc-results.png" /></p>
<p>It should be noted that the precision-recall curves are skewed because the data is itself skewed; around 70% of all ratings in the data are 4 stars or above. The precision-recall curve suggests that, using one of the high-performing models, we can attempt to require that 90% of all recommendations are for actually relevant and still recommend around 70% of all actually relevant items. From the perspective of an online retailer looking to surface more of its product catalog, this would be an encouraging result.</p>
<h4 id="performance-on-prolific-users"><a name="user-content-performance-on-prolific-users" href="#performance-on-prolific-users" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Performance on prolific users</h4>
<p>The hybrid scatter and line plots below visualize how a model&rsquo;s performance changes for users based on how many ratings they have made. Each point of the scatter plot represents a user, defined by how many ratings that user has made and the MAE for that user. The red line delineates the average MAE of all users per ratings count.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/users-results.png" /></p>
<p>Unsurprisingly, across all models, a common pattern is that the MAE has high variance when the number of reviews per user is low, and converges as the number of reviews increases. However, it appears that, on average, the models do not perform better for more prolific users.</p>
<h4 id="performance-on-popular-items"><a name="user-content-performance-on-popular-items" href="#performance-on-popular-items" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Performance on popular items</h4>
<p>A similar analysis of how a model&rsquo;s performance changes for items based on how many times it has been rated yields similar results: on average, the models do not perform better for more popular items.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/items-results.png" /></p>
<h4 id="performance-by-release-year"><a name="user-content-performance-by-release-year" href="#performance-by-release-year" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Performance by Release Year</h4>
<p>Interestingly, starting from 1920, all models (with the exception of the mean baseline) tend to have higher MAE&rsquo;s for items released more recently. The MAE values are quite unstable before the year 1920, presumably due to the scarcity of items of that era in Amazon&rsquo;s catalog.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/year-results.png" /></p>
<p>A possible explanation for this trend is that as the number of items in Amazon&rsquo;s catalog increases, the ratings data surrounding these items becomes noisier. Moreover, it may also be the case that people who like &ldquo;classic&rdquo; older films have more specific tastes that may be easier to account for.</p>
<h4 id="performance-by-mpaa-rating"><a name="user-content-performance-by-mpaa-rating" href="#performance-by-mpaa-rating" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Performance by MPAA Rating</h4>
<p>Another surprising finding is that MAE tends to increase for items intended for increasingly mature audiences, with PG-rated films having the lowest MAE and R and NC-17 rated films having the highest. This suggests that more family-friendly entertainment may be easier to predict ratings for.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/mpaa-results.png" /></p>
<h4 id="lower-bounds-for-catalog-and-user-coverage"><a name="user-content-lower-bounds-for-catalog-and-user-coverage" href="#lower-bounds-for-catalog-and-user-coverage" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Lower Bounds for Catalog and User Coverage</h4>
<p>Calculating catalog coverage is a computationally expensive process wherein predictions must be made for all unrated user-item combinations, and the proportion of items that have at least one predicted rating of 4 or above is calculated. Similarly, user coverage can be thought of as the proportion of all users that have at least one predicted rating of 4 or above. Because of limited computational resources, in lieu of calculating exact catalog coverage, we calculate a lower bound by using only predicted ratings for the cross-validation and test sets rather than predicted ratings for the whole universe of user-item pairs. Note that because the average training set rating is 3.9, the mean baseline effectively predicts no items are relevant for any users, resulting in catalog and user coverage of 0.</p>
<table>
<thead>
<tr>
<th></th>
<th>Mean Baseline</th>
<th>CF with LSH</th>
<th>Bias Baseline</th>
<th>FM with RatingsOnly</th>
<th>FM with LSH+Bias</th>
<th>FM with ExternalData</th>
<th>GBT Ensemble</th>
</tr>
</thead>
<tbody>
<tr>
<td>Catalog coverage</td>
<td>0.0</td>
<td>0.963784</td>
<td>0.856615</td>
<td>0.874945</td>
<td>0.874353</td>
<td>0.874058</td>
<td>0.893570</td>
</tr>
<tr>
<td>User coverage</td>
<td>0.0</td>
<td>0.700719</td>
<td>0.807672</td>
<td>0.824720</td>
<td>0.824987</td>
<td>0.837240</td>
<td>0.843767</td>
</tr>
</tbody>
</table>
<p>It should also be noted that user coverage is a less important metric since if a recommendation system cannot predict ratings of 4 or above on any item for a specific user, it can simply recommend the top <em>k</em> items with the highest predicted ratings. However, it is still useful to know a lower bound for the proportion of users for which the system can recommend items it believes are relevant.</p>
<p><img alt="" src="/Users/derekzhao/columbia/ieor-4571-personalization/project/part-02/imgs/coverage-results.png" /></p>
<p>With the exception of the mean baseline, all models have a catalog coverage rate of above 0.8, and with the exception of the mean baseline and CF with LSH, all models have a user coverage of above 0.8 as well. Notably, the GBT ensemble performs best in terms of both catalog and user coverage.</p>
<h2 id="conclusion"><a name="user-content-conclusion" href="#conclusion" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Conclusion</h2>
<p>In Part II of this project, we implemented our own versions of probabilistic latent semantic indexing and collaborative filtering with cosine-based locality sensitive hashing. We then tested the performance of CF with cosine-based LSH, PLSI, FM&rsquo;s with various forms of feature engineering, and ensembling techniques for predicting user-item star ratings in Amazon&rsquo;s film and TV product catalog and compared the performance against common baseline models.</p>
<p>Based on the tests and analyses performed, we observed that all FM models outperform the bias baseline, with the FM model that incorporates scraped external data and predictions from the collaborative filtering and bias baseline model yielding the lowest MAE and highest AUC score. The results show that the incorporation of additional features and predictions from other models into an FM does indeed improve accuracy in predicting whether an item is relevant to a user.</p>
<p>As a practical matter, however, the improvement in performance gained from constructing these additional features does not appear to justify the effort required to collect, process, and tune the data for any setting other than a competitive one (ex: Kaggle). Thus, in a corporate setting, we would recommend the use of an FM using only basic ratings data as it is simple to implement, incredibly fast, and more accurate than other methods.</p></article></body></html>